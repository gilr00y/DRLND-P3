{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import envs\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "# env.close()\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size # 2\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1] # 24\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     10,
     15,
     34
    ]
   },
   "outputs": [],
   "source": [
    "dim_actor_in = state_size # 24 observations per agent\n",
    "dim_actor_h1 = 128\n",
    "dim_actor_h2 = 128\n",
    "dim_actor_out = action_size\n",
    "\n",
    "dim_critic_in = state_size + dim_actor_out\n",
    "dim_critic_h1 = 128\n",
    "dim_critic_h2 = 128\n",
    "dim_critic_out = 1\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_actor_in, dim_actor_h1)\n",
    "        self.bn1 = nn.BatchNorm1d(dim_actor_h1)\n",
    "        self.fc2 = nn.Linear(dim_actor_h1, dim_actor_h2)\n",
    "        self.bn2 = nn.BatchNorm1d(dim_actor_h2)\n",
    "        self.fc3 = nn.Linear(dim_actor_h2, dim_actor_out)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        h1 = self.bn1(\n",
    "            f.relu(\n",
    "                self.fc1(state)))\n",
    "        h2 = self.bn2(\n",
    "            f.relu(\n",
    "                self.fc2(h1)))\n",
    "        out = self.fc3(h2)\n",
    "        return out\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_critic_in, dim_critic_h1)\n",
    "        self.bn1 = nn.BatchNorm1d(dim_critic_h1)\n",
    "        self.fc2 = nn.Linear(dim_critic_h1, dim_critic_h2)\n",
    "        self.bn2 = nn.BatchNorm1d(dim_critic_h2)\n",
    "        self.fc3 = nn.Linear(dim_critic_h2, dim_critic_out)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "#         print(\"STATE\", len(state))\n",
    "#         print(state)\n",
    "#         print(\"ACtion\", len(action))\n",
    "#         print(action)\n",
    "#         print(torch.cat((state, action), dim=1))\n",
    "#         raise\n",
    "        x = torch.cat((state, action), dim=1)\n",
    "        h1 = self.bn1(\n",
    "            f.relu(\n",
    "                self.fc1(x)))\n",
    "        h2 = self.bn2(\n",
    "            f.relu(\n",
    "                self.fc2(h1)))\n",
    "        out = self.fc3(h2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2000\n",
    "buffer_size = int(1e5)\n",
    "batch_size = 128\n",
    "num_agents = 2\n",
    "gamma = 1\n",
    "\n",
    "tau = 1e-3\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-4\n",
    "\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.actor_local = Actor()\n",
    "        self.actor_target = Actor()\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=lr_actor)\n",
    "        \n",
    "        self.critic_local = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=lr_critic)\n",
    "        self.replay_buffer = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "        self.tau = tau\n",
    "        \n",
    "    def act(self, state):\n",
    "        self.actor_local.eval()\n",
    "        action = self.actor_local(state)\n",
    "        self.actor_local.train()\n",
    "        return action.data.numpy()\n",
    "        \n",
    "    def _update_actor(self):\n",
    "        for target_param, local_param in zip(self.actor_target.parameters(), self.actor_local.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def _update_critic(self):\n",
    "        for target_param, local_param in zip(self.critic_target.parameters(), self.critic_local.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def soft_update(self):\n",
    "        self._update_critic()\n",
    "        self._update_actor()\n",
    "\n",
    "class Agency:\n",
    "    def __init__(self):\n",
    "        self.agents = [Agent() for _ in range(num_agents)]\n",
    "        \n",
    "    def act(self, states):\n",
    "        states = states.reshape(-1, 1, 24)\n",
    "        actions = np.array([\n",
    "            agent.act(torch.from_numpy(states[idx]).float())[0] for idx, agent in enumerate(self.agents)\n",
    "        ])\n",
    "        print(actions)\n",
    "        return actions\n",
    "    \n",
    "    def add_experience(self, states,actions,rewards,next_states,dones):\n",
    "        [agent.replay_buffer.add(\n",
    "            states[i],\n",
    "            actions[i],\n",
    "            rewards[i],\n",
    "            next_states[i],\n",
    "            dones[i]) for i, agent in enumerate(self.agents)]\n",
    "    \n",
    "    def train(self):\n",
    "        for idx, agent in enumerate(self.agents):\n",
    "            if len(agent.replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = agent.replay_buffer.sample()\n",
    "\n",
    "                # UPDATE CRITIC\n",
    "                # Get predicted next-state actions and Q values from target models\n",
    "                actions_future = agent.actor_target(next_states)\n",
    "                Q_future = agent.critic_target(next_states, actions_future)\n",
    "                # Compute Q targets for current states (y_i)\n",
    "                Q_targets = rewards + (gamma * Q_future * (1 - dones))\n",
    "                # Compute critic loss\n",
    "                Q_expected = agent.critic_local(states, actions)\n",
    "                critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "                # Minimize the loss\n",
    "                agent.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                agent.critic_optimizer.step()\n",
    "                # Compute actor loss\n",
    "                actions_pred = agent.actor_local(states)\n",
    "                # Why is this the actor loss??\n",
    "                actor_loss = -agent.critic_local(states, actions_pred).mean()\n",
    "                # Minimize the loss\n",
    "                agent.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                agent.actor_optimizer.step()\n",
    "                # ----------------------- update target networks ----------------------- #\n",
    "                agent.soft_update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15807253 0.09351274]\n",
      " [0.03112925 0.13984054]]\n",
      "[[ 0.2568594   0.01911046]\n",
      " [ 0.12634036 -0.0060163 ]]\n",
      "[[0.37548676 0.00226782]\n",
      " [0.15050364 0.07974904]]\n",
      "[[ 0.39029795 -0.03275769]\n",
      " [ 0.16444992  0.07585903]]\n",
      "[[ 0.35334495 -0.0421446 ]\n",
      " [ 0.16511345  0.13771847]]\n",
      "[[ 0.3532662  -0.05433065]\n",
      " [ 0.21488228  0.13628966]]\n",
      "[[ 0.36166382 -0.06565345]\n",
      " [ 0.23917052  0.14414153]]\n",
      "[[ 0.34535202 -0.06292668]\n",
      " [ 0.26467234  0.1421898 ]]\n",
      "[[ 0.34280702 -0.05602814]\n",
      " [ 0.2977041   0.15513739]]\n",
      "[[ 0.34341702 -0.04454149]\n",
      " [ 0.3192769   0.17654648]]\n",
      "[[ 0.3264316  -0.02305779]\n",
      " [ 0.36018437  0.18838271]]\n",
      "[[ 0.31367594 -0.00358091]\n",
      " [ 0.40076306  0.20796168]]\n",
      "[[0.30326033 0.01524048]\n",
      " [0.43557358 0.2224566 ]]\n",
      "[[0.29336438 0.0327353 ]\n",
      " [0.4673375  0.23401475]]\n",
      "[[0.288622   0.05236665]\n",
      " [0.48939127 0.24737161]]\n",
      "EPISODE=0\tSCORE=0.0\tAVG=0.0[[0.14319324 0.08408098]\n",
      " [0.03123207 0.1411131 ]]\n",
      "[[ 0.23101589  0.00805628]\n",
      " [ 0.1251458  -0.00342022]]\n",
      "[[ 0.35579398 -0.016828  ]\n",
      " [ 0.14716738  0.0825299 ]]\n",
      "[[ 0.38216248 -0.04842456]\n",
      " [ 0.1583021   0.0776675 ]]\n",
      "[[ 0.35065928 -0.05024537]\n",
      " [ 0.15829322  0.13960013]]\n",
      "[[ 0.34180245 -0.05876514]\n",
      " [ 0.20591947  0.13650487]]\n",
      "[[ 0.35249466 -0.0694866 ]\n",
      " [ 0.22720858  0.14369181]]\n",
      "[[ 0.34050378 -0.05613049]\n",
      " [ 0.24890643  0.14152224]]\n",
      "[[ 0.33508614 -0.0463495 ]\n",
      " [ 0.27729046  0.15694624]]\n",
      "[[ 0.32809135 -0.02866635]\n",
      " [ 0.30187187  0.17770056]]\n",
      "[[ 0.31240067 -0.00683318]\n",
      " [ 0.34221482  0.18899138]]\n",
      "[[0.29936364 0.013038  ]\n",
      " [0.3818476  0.20166296]]\n",
      "[[0.29092422 0.02956419]\n",
      " [0.4144974  0.2153686 ]]\n",
      "[[0.28562576 0.04904135]\n",
      " [0.44426444 0.22745174]]\n",
      "EPISODE=1\tSCORE=0.0\tAVG=0.0[[0.15416121 0.09154762]\n",
      " [0.0288082  0.15130222]]\n",
      "[[0.24863863 0.02004808]\n",
      " [0.11769628 0.0028975 ]]\n",
      "[[0.36380085 0.00335252]\n",
      " [0.13425952 0.09225082]]\n",
      "[[ 0.37888014 -0.03131465]\n",
      " [ 0.13990784  0.08554982]]\n",
      "[[ 0.3421401  -0.04023018]\n",
      " [ 0.13910313  0.14251494]]\n",
      "[[ 0.34129074 -0.05001991]\n",
      " [ 0.18268642  0.13870987]]\n",
      "[[ 0.34898487 -0.06057183]\n",
      " [ 0.20150892  0.14314488]]\n",
      "[[ 0.3327501  -0.05591066]\n",
      " [ 0.21648207  0.14142704]]\n",
      "[[ 0.32798633 -0.04738737]\n",
      " [ 0.23859504  0.1515989 ]]\n",
      "[[ 0.326293   -0.03252546]\n",
      " [ 0.26078016  0.16304466]]\n",
      "[[ 0.3090651  -0.01004509]\n",
      " [ 0.28291282  0.17945197]]\n",
      "[[0.2956417  0.00984838]\n",
      " [0.3213027  0.19254744]]\n",
      "[[0.2876627  0.0263667 ]\n",
      " [0.35992265 0.1971528 ]]\n",
      "[[0.28185079 0.04556995]\n",
      " [0.3931235  0.20611608]]\n",
      "EPISODE=2\tSCORE=0.0\tAVG=0.0[[0.14698297 0.08373494]\n",
      " [0.02991135 0.13354254]]\n",
      "[[ 0.24501717 -0.007994  ]\n",
      " [ 0.13004681 -0.01591648]]\n",
      "[[ 0.3888087  -0.03478808]\n",
      " [ 0.15615049  0.06950702]]\n",
      "[[ 0.4177966  -0.06333356]\n",
      " [ 0.17965168  0.06950203]]\n",
      "[[ 0.38830492 -0.06136701]\n",
      " [ 0.1798017   0.12740526]]\n",
      "[[ 0.37912217 -0.06704321]\n",
      " [ 0.23266613  0.12565207]]\n",
      "[[ 0.3916825  -0.07852789]\n",
      " [ 0.2601086   0.13924105]]\n",
      "[[ 0.3810623  -0.06539687]\n",
      " [ 0.2878989   0.14252964]]\n",
      "[[ 0.37590018 -0.05597606]\n",
      " [ 0.32394418  0.16199923]]\n",
      "[[ 0.36947596 -0.03925466]\n",
      " [ 0.3538021   0.18621609]]\n",
      "[[ 0.3544385  -0.01835863]\n",
      " [ 0.40128592  0.1961973 ]]\n",
      "[[0.3410037  0.00256891]\n",
      " [0.44512513 0.21576652]]\n",
      "[[0.3301163  0.01865872]\n",
      " [0.48355374 0.23047057]]\n",
      "[[0.3231802  0.03630442]\n",
      " [0.5181602  0.24339306]]\n",
      "EPISODE=3\tSCORE=0.0\tAVG=0.0[[0.14866501 0.08911924]\n",
      " [0.02780173 0.16107059]]\n",
      "[[0.23681453 0.02353556]\n",
      " [0.11329492 0.01273728]]\n",
      "[[0.34487566 0.00655686]\n",
      " [0.12626345 0.10163718]]\n",
      "[[ 0.364663   -0.02998697]\n",
      " [ 0.1261904   0.09643143]]\n",
      "[[ 0.32758242 -0.0395627 ]\n",
      " [ 0.12443846  0.14986226]]\n",
      "[[ 0.3221963  -0.04986883]\n",
      " [ 0.16230744  0.14590475]]\n",
      "[[ 0.33333424 -0.06183651]\n",
      " [ 0.17937925  0.14750059]]\n",
      "[[ 0.31839624 -0.05744461]\n",
      " [ 0.18791345  0.14704937]]\n",
      "[[ 0.31460512 -0.05013276]\n",
      " [ 0.2077119   0.15882328]]\n",
      "[[ 0.31426558 -0.03787681]\n",
      " [ 0.22331202  0.15731925]]\n",
      "[[ 0.29762992 -0.01643012]\n",
      " [ 0.23501074  0.16417605]]\n",
      "[[0.28426543 0.00333903]\n",
      " [0.25487417 0.18066812]]\n",
      "[[0.27430114 0.02200415]\n",
      " [0.28941604 0.19417328]]\n",
      "[[0.26705533 0.04002891]\n",
      " [0.32171404 0.19356862]]\n",
      "EPISODE=4\tSCORE=0.0\tAVG=0.0[[0.15782303 0.10606106]\n",
      " [0.04682967 0.23624104]]\n",
      "[[0.23248932 0.02474464]\n",
      " [0.03140429 0.30521154]]\n",
      "[[ 0.28014752  0.02473964]\n",
      " [-0.01421954  0.478172  ]]\n",
      "[[ 0.24883452  0.03554829]\n",
      " [-0.00833217  0.4333892 ]]\n",
      "[[ 0.20262742 -0.02085645]\n",
      " [-0.0267845   0.43846112]]\n",
      "[[ 0.20986703 -0.05590452]\n",
      " [-0.02492643  0.43116054]]\n",
      "[[ 0.20455998 -0.05024328]\n",
      " [-0.02651683  0.42066744]]\n",
      "[[ 0.19628054 -0.03457876]\n",
      " [-0.03350397  0.412959  ]]\n",
      "[[ 0.19489312 -0.03426654]\n",
      " [-0.04638032  0.39906934]]\n",
      "[[ 0.19678524 -0.01646553]\n",
      " [-0.06258716  0.38382307]]\n",
      "[[ 0.19944736  0.00580014]\n",
      " [-0.08092872  0.36746886]]\n",
      "[[ 0.20297992  0.020239  ]\n",
      " [-0.10415649  0.354554  ]]\n",
      "[[ 0.20316088  0.04385637]\n",
      " [-0.12867528  0.33915898]]\n",
      "[[ 0.20511964  0.06892008]\n",
      " [-0.14502531  0.31848612]]\n",
      "[[ 0.20309988  0.09669094]\n",
      " [-0.15410224  0.29901555]]\n",
      "EPISODE=5\tSCORE=0.0\tAVG=0.0[[0.15559703 0.09132686]\n",
      " [0.03159418 0.13331029]]\n",
      "[[ 0.252616    0.01241652]\n",
      " [ 0.12952572 -0.00991713]]\n",
      "[[ 0.37885123 -0.00659376]\n",
      " [ 0.15539324  0.07395756]]\n",
      "[[ 0.39402533 -0.03872535]\n",
      " [ 0.17286089  0.07216018]]\n",
      "[[ 0.3596916  -0.04409831]\n",
      " [ 0.17279074  0.13412288]]\n",
      "[[ 0.35784376 -0.05397355]\n",
      " [ 0.22180572  0.13297659]]\n",
      "[[ 0.36501858 -0.06199089]\n",
      " [ 0.24777353  0.14213067]]\n",
      "[[ 0.34935883 -0.0566528 ]\n",
      " [ 0.2740816   0.14324877]]\n",
      "[[ 0.34336725 -0.04535694]\n",
      " [ 0.30527008  0.16317807]]\n",
      "[[ 0.34054896 -0.03014406]\n",
      " [ 0.33377385  0.18524483]]\n",
      "[[ 0.32355243 -0.00725371]\n",
      " [ 0.3800775   0.19550359]]\n",
      "[[0.3105155  0.01222851]\n",
      " [0.4180111  0.21473569]]\n",
      "[[0.3030525 0.0278298]\n",
      " [0.4509786 0.2247614]]\n",
      "[[0.29759282 0.04725894]\n",
      " [0.48023525 0.2387912 ]]\n",
      "EPISODE=6\tSCORE=0.0\tAVG=0.0[[0.15253314 0.09874547]\n",
      " [0.0466645  0.22024068]]\n",
      "[[0.21385074 0.03112845]\n",
      " [0.03831331 0.27423847]]\n",
      "[[ 2.6678914e-01  3.6300607e-02]\n",
      " [-1.4718622e-04  4.3695343e-01]]\n",
      "[[0.22931427 0.04249028]\n",
      " [0.00279292 0.39228854]]\n",
      "[[ 0.18211812 -0.01026886]\n",
      " [-0.01338948  0.39889294]]\n",
      "[[ 0.19694635 -0.03795003]\n",
      " [-0.00684715  0.39400274]]\n",
      "[[ 0.1870541  -0.03146628]\n",
      " [-0.00613942  0.3835929 ]]\n",
      "[[ 0.17652687 -0.01996482]\n",
      " [-0.01340835  0.37507072]]\n",
      "[[ 0.17310488 -0.02260693]\n",
      " [-0.02295726  0.3578904 ]]\n",
      "[[ 0.17213884 -0.00580618]\n",
      " [-0.03860847  0.3397909 ]]\n",
      "[[ 0.17469507  0.01436079]\n",
      " [-0.06110793  0.32635257]]\n",
      "[[ 0.17777738  0.03335185]\n",
      " [-0.08532559  0.31324264]]\n",
      "[[ 0.1763944   0.05565056]\n",
      " [-0.10519122  0.2968873 ]]\n",
      "[[ 0.17490971  0.08165945]\n",
      " [-0.11817238  0.27234364]]\n",
      "EPISODE=7\tSCORE=0.0\tAVG=0.0[[0.15386882 0.09183989]\n",
      " [0.02944345 0.15003619]]\n",
      "[[0.24752933 0.02355852]\n",
      " [0.11876298 0.00386772]]\n",
      "[[0.35777974 0.00797094]\n",
      " [0.13756752 0.09107977]]\n",
      "[[ 0.37352055 -0.02769887]\n",
      " [ 0.14285031  0.08577734]]\n",
      "[[ 0.3353801  -0.03867481]\n",
      " [ 0.14268452  0.14446095]]\n",
      "[[ 0.33414474 -0.04870693]\n",
      " [ 0.18603     0.1415222 ]]\n",
      "[[ 0.3429375  -0.05968262]\n",
      " [ 0.20624927  0.14536431]]\n",
      "[[ 0.32627228 -0.05647627]\n",
      " [ 0.2240079   0.14394204]]\n",
      "[[ 0.32257512 -0.04905327]\n",
      " [ 0.2461639   0.1525439 ]]\n",
      "[[ 0.32191047 -0.03544824]\n",
      " [ 0.2682009   0.16305871]]\n",
      "[[ 0.30451044 -0.01347195]\n",
      " [ 0.29047626  0.18151191]]\n",
      "[[0.29130605 0.00636335]\n",
      " [0.33245465 0.19184652]]\n",
      "[[0.28185132 0.02406248]\n",
      " [0.3678706  0.20051432]]\n",
      "[[0.27601874 0.04240263]\n",
      " [0.39858106 0.21066082]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_update_critic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c2d2297c401c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0magency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0magency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Environment Rules take the max of the agents as the \"episode score\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mep_scores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-337d5faa1d3b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;31m# ----------------------- update target networks ----------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-337d5faa1d3b>\u001b[0m in \u001b[0;36msoft_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0m_update_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0m_update_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_update_critic' is not defined"
     ]
    }
   ],
   "source": [
    "scores = np.array([]) \n",
    "\n",
    "# Collection of agents\n",
    "agency = Agency()\n",
    "\n",
    "for ep_num in range(num_episodes):\n",
    "    ep_scores = np.zeros(num_agents)\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    while True:\n",
    "        states = env_info.vector_observations\n",
    "        actions = agency.act(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        agency.add_experience(states,actions,rewards,next_states,dones)\n",
    "        agency.train() \n",
    "        # Environment Rules take the max of the agents as the \"episode score\"\n",
    "        ep_scores += rewards\n",
    "        if sum(dones) > 0:\n",
    "            break\n",
    "    ep_score = np.max(ep_scores)\n",
    "    scores = np.append(scores, ep_score)\n",
    "    print(\"\\rEPISODE={}\\tSCORE={}\\tAVG={}\".format(ep_num, ep_score, np.mean(scores[-100:])), end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
